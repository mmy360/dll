# CV
| **Impl** | **Abbreviation**          | **Conference/Year**                      | **Paper Title**                                                        | **Author**                                                         | **Link**                                                   | **Contribution**                                                 |
|----------|----------------------|---------------------------------|-------------------------------------------------------------------|------------------------------------------------------------------|-------------------------------------------------------------|------------------------------------------------------------|
| ✅        | **LeNet-5**          | IEEE 1998                      | Gradient-Based Learning Applied to Document Recognition            | Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner           | [Link](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)  | 开创了使用卷积神经网络（CNN）进行手写数字识别。                |
| ✅        | **AlexNet**          | NeurIPS 2012                   | ImageNet Classification with Deep Convolutional Neural Networks    | Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton                 | [Link](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) | 在ImageNet竞赛中取得突破性成绩，大幅提高了分类准确性。             |
|          | **VGGNet**           | ICLR 2015                      | Very Deep Convolutional Networks for Large-Scale Image Recognition | Karen Simonyan, Andrew Zisserman                                  | [Link](https://arxiv.org/pdf/1409.1556.pdf)                 | 通过增加网络深度改进分类性能，引入了VGG架构。                    |
|          | **GoogLeNet (Inception V1)** | CVPR 2015               | Going Deeper with Convolutions                                     | Christian Szegedy, Wojciech Zaremba, Sergey Ioffe                | [Link](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf) | 引入了Inception模块，提高了计算效率和分类性能。                  |
| ✅        | **ResNet**           | CVPR 2016                      | Deep Residual Learning for Image Recognition                       | Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun                | [Link](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) | 通过残差连接解决深度网络中的梯度消失问题，提高了模型性能。            |
|          | **DenseNet**         | CVPR 2017                      | Densely Connected Convolutional Networks                           | Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger | [Link](https://arxiv.org/pdf/1608.06993.pdf)                | 引入了密集连接层，改善了梯度流和特征复用。                        |
|          | **MobileNet V2**     | CVPR 2018                      | MobileNetV2: Inverted Residuals and Linear Bottlenecks             | Mark Sandler, Andrew Howard, Menglong Zhu, Li Zhang, etc.        | [Link](https://arxiv.org/pdf/1801.04381.pdf)                | 通过线性瓶颈和倒置残差提高了移动应用的效率和性能。                 |
|          | **MobileNet V3**     | CVPR 2019                      | MobileNetV3: When Efficiency Meets Accuracy                        | Andrew G. Howard, Mark Sandler, Grace Chu, et al.                | [Link](https://arxiv.org/pdf/1905.02244.pdf)                | 通过优化提高了MobileNet架构的准确性和效率。                     |
|          | **EfficientNet**     | ICML 2019                      | EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks | Mingxing Tan, Quoc V. Le                                     | [Link](https://arxiv.org/pdf/1905.11946.pdf)                | 通过复合缩放方法提高了分类性能和计算效率。                       |
|          | **Vision Transformers (ViT)** | NeurIPS 2020          | An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale | Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov       | [Link](https://arxiv.org/pdf/2010.11929.pdf)                | 将transformer应用于图像分类任务，打破了传统CNN的限制。             |
|          | **Swin Transformer** | CVPR 2021                      | Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | Ze Liu, Yutong Lin, Yue Cao, Han Hu, and others            | [Link](https://arxiv.org/pdf/2103.14030.pdf)                | 通过使用平移窗口的层次结构Vision Transformer改进了精度和效率。         |

---

# NLP

| **Impl** | **Abbreviation**          | **Conference/Year**                      | **Paper Title**                                                        | **Author**                                                         | **Link**                                                   | **Contribution**                                                 |
|----------|----------------------|---------------------------------|-------------------------------------------------------------------|------------------------------------------------------------------|-------------------------------------------------------------|------------------------------------------------------------|
| ✅        | **RNN**              | IEEE 1986                      | Learning Representations by Back-Propagating Errors                | David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams        | [Link](https://www.nature.com/articles/323533a0)            | 提出了通过反向传播训练递归神经网络（RNN）的方法，开创了序列建模的新方向。 |
| ✅        | **LSTM**             | Neural Computation 1997         | Long Short-Term Memory                                             | Sepp Hochreiter, Jürgen Schmidhuber                              | [Link](https://www.bioinf.jku.at/publications/older/2604.pdf) | 通过引入记忆单元和门控机制，解决了RNN中的梯度消失问题，极大提高了长序列建模能力。 |
| ✅        | **GRU**              | arXiv 2014                      | Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation | Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, et al.   | [Link](https://arxiv.org/pdf/1406.1078.pdf)                | 提出了门控循环单元（GRU），作为LSTM的简化版本，保留了性能的同时减少了计算复杂度。 |
| ✅        | **Word2Vec**         | NIPS 2013                       | Efficient Estimation of Word Representations in Vector Space       | Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean              | [Link](https://arxiv.org/pdf/1301.3781.pdf)                | 提出了Word2Vec模型，通过词向量表示大幅提升了词语之间语义关系的捕捉能力。       |
| ✅        | **GloVe**            | EMNLP 2014                      | GloVe: Global Vectors for Word Representation                      | Jeffrey Pennington, Richard Socher, Christopher D. Manning       | [Link](https://www.aclweb.org/anthology/D14-1162.pdf)       | 提出了GloVe模型，结合了全局统计信息和局部上下文窗口，提高了词向量表示的准确性。  |
| ✅        | **Seq2Seq**          | NIPS 2014                       | Sequence to Sequence Learning with Neural Networks                 | Ilya Sutskever, Oriol Vinyals, Quoc V. Le                        | [Link](https://arxiv.org/pdf/1409.3215.pdf)                | 提出了Seq2Seq模型框架，极大推动了机器翻译、摘要生成等序列生成任务的发展。      |
| ✅        | **Attention**        | NeurIPS 2017                    | Attention is All You Need                                          | Ashish Vaswani, Noam Shazeer, Niki Parmar, et al.                | [Link](https://arxiv.org/pdf/1706.03762.pdf)               | 引入了自注意力机制，提出了Transformer架构，显著提高了NLP任务的性能和效率。     |
| ✅        | **BERT**             | NAACL 2019                     | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova | [Link](https://arxiv.org/pdf/1810.04805.pdf)               | 引入了双向Transformer模型，显著提升了NLP任务的性能。               |
| ✅        | **GPT-2**            | OpenAI 2019                    | Language Models are Unsupervised Multitask Learners                 | Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei | [Link](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) | 展示了大规模语言模型的生成能力，推动了NLP生成任务的发展。           |
| ✅        | **T5**               | JMLR 2020                      | Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer | Colin Raffel, Noam Shazeer, Adam Roberts, et al.             | [Link](https://arxiv.org/pdf/1910.10683.pdf)               | 提出了一个统一的文本到文本框架，显著提升了多种NLP任务的性能。        |
| ✅        | **RoBERTa**          | ArXiv 2019                     | RoBERTa: A Robustly Optimized BERT Pretraining Approach             | Yinhan Liu, Myle Ott, Naman Goyal, et al.                      | [Link](https://arxiv.org/pdf/1907.11692.pdf)               | 对BERT的预训练过程进行了优化，提高了模型的性能和泛化能力。          |
| ✅        | **T5**               | JMLR 2020                      | Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer | Colin Raffel, Noam Shazeer, Adam Roberts, et al.             | [Link](https://arxiv.org/pdf/1910.10683.pdf)               | 提出了一个统一的文本到文本框架，显著提升了多种NLP任务的性能。        |
|          | **GPT-3**            | NeurIPS 2020                   | Language Models are Few-Shot Learners                                | Tom B. Brown, Benjamin Mann, Nick Ryder, et al.               | [Link](https://arxiv.org/pdf/2005.14165.pdf)               | 展示了语言模型在少样本学习中的强大能力，推动了大型预训练模型的发展。 |
|          | **T5.1.1**           | ArXiv 2021                     | T5.1.1: Understanding of Few-Shot, Multitask, and Transfer Learning for Text Generation | Colin Raffel, Noam Shazeer, Adam Roberts, et al.             | [Link](https://arxiv.org/pdf/1910.10683v3.pdf)             | 通过改进模型和预训练方法，提高了T5在文本生成任务中的性能。          |
|          | **BART**             | ACL 2020                       | BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension | Mike Lewis, Yinhan Liu, Naman Goyal, et al.                  | [Link](https://arxiv.org/pdf/1910.13461.pdf)               | 提出了用于文本生成和翻译任务的去噪序列到序列预训练模型。              |
|          | **T5.1.1**           | ArXiv 2021                     | T5.1.1: Understanding of Few-Shot, Multitask, and Transfer Learning for Text Generation | Colin Raffel, Noam Shazeer, Adam Roberts, et al.             | [Link](https://arxiv.org/pdf/1910.10683v3.pdf)             | 通过改进模型和预训练方法，提高了T5在文本生成任务中的性能。          |
|          | **Switch Transformer** | JMLR 2021                     | Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity | William Fedus, Barret Zoph, Noam Shazeer, et al.            | [Link](https://arxiv.org/pdf/2101.03961.pdf)               | 提出了一个高效的稀疏Transformer架构，大幅降低了训练和推理成本。        |

